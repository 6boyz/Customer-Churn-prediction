{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, f1_score, precision_score, roc_auc_score\n",
    "\n",
    "TARGET = \"alive\"\n",
    "PRED = \"pred_alive\"\n",
    "\n",
    "\n",
    "def calc_destribution(data: dict) -> dict:\n",
    "    data_sum = sum(data.values())\n",
    "    distribution = {}\n",
    "    for key, value in data.items():\n",
    "        distribution[key] = f\"{round(value / data_sum, 3) * 100:.1f}%\"\n",
    "    return distribution\n",
    "\n",
    "def data_balancing_by_col(data_any: pd.DataFrame, targ_col: str = TARGET) -> pd.DataFrame:\n",
    "    val_cnt = data_any[targ_col].value_counts()\n",
    "    minimal_counts = val_cnt.min()\n",
    "    cols = val_cnt.index.to_list()\n",
    "    balanced_data_dfs = []\n",
    "    for col_val in cols: balanced_data_dfs.append(data_any[data_any[targ_col] == col_val].sample(minimal_counts))\n",
    "    return pd.concat(balanced_data_dfs)\n",
    "\n",
    "def calc_metrics(data_any: pd.DataFrame, target: str = TARGET, predicted: str = PRED) -> tuple[float]:\n",
    "    recall = recall_score(data_any[target], data_any[predicted])\n",
    "    f1 = f1_score(data_any[target], data_any[predicted])\n",
    "    precision = precision_score(data_any[target], data_any[predicted])\n",
    "    roc_auc = roc_auc_score(data_any[target], data_any[predicted])\n",
    "    print(f\"recall: {recall:.3f}, f1: {f1:.3f}, precision: {precision:.3f}, roc_auc: {roc_auc:.3f}\")\n",
    "    return (recall, f1, precision, roc_auc)\n",
    "\n",
    "def calc_predictions_by_days(data_rfm: pd.DataFrame, days_for_die: int = 10) -> pd.DataFrame:\n",
    "    local_data = data_rfm.copy()\n",
    "    last_data_date = local_data['first_buy'].max()\n",
    "    local_data[PRED] = local_data['last_buy'].apply(lambda x: (last_data_date - x).days < days_for_die)\n",
    "    return local_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days: 30: recall: 0.745, f1: 0.757, precision: 0.768, roc_auc: 0.760\n",
      "Days: 31: recall: 0.748, f1: 0.757, precision: 0.766, roc_auc: 0.760\n",
      "Days: 32: recall: 0.750, f1: 0.758, precision: 0.766, roc_auc: 0.760\n",
      "Days: 33: recall: 0.753, f1: 0.758, precision: 0.763, roc_auc: 0.760\n",
      "Days: 34: recall: 0.755, f1: 0.759, precision: 0.762, roc_auc: 0.760\n",
      "Days: 35: recall: 0.757, f1: 0.759, precision: 0.760, roc_auc: 0.759\n",
      "Days: 36: recall: 0.759, f1: 0.759, precision: 0.759, roc_auc: 0.759\n",
      "Days: 37: recall: 0.761, f1: 0.760, precision: 0.758, roc_auc: 0.759\n",
      "Days: 38: recall: 0.762, f1: 0.760, precision: 0.758, roc_auc: 0.759\n",
      "Days: 39: recall: 0.765, f1: 0.760, precision: 0.756, roc_auc: 0.759\n",
      "Days: 40: recall: 0.775, f1: 0.763, precision: 0.751, roc_auc: 0.759\n",
      "Days: 41: recall: 0.788, f1: 0.767, precision: 0.747, roc_auc: 0.760\n",
      "Days: 42: recall: 0.797, f1: 0.769, precision: 0.742, roc_auc: 0.760\n",
      "Days: 43: recall: 0.807, f1: 0.770, precision: 0.737, roc_auc: 0.759\n",
      "Days: 44: recall: 0.808, f1: 0.770, precision: 0.736, roc_auc: 0.759\n",
      "Days: 45: recall: 0.811, f1: 0.770, precision: 0.733, roc_auc: 0.758\n",
      "Days: 46: recall: 0.819, f1: 0.771, precision: 0.729, roc_auc: 0.757\n",
      "Days: 47: recall: 0.824, f1: 0.771, precision: 0.725, roc_auc: 0.756\n",
      "Days: 48: recall: 0.829, f1: 0.771, precision: 0.721, roc_auc: 0.754\n",
      "Days: 49: recall: 0.832, f1: 0.771, precision: 0.717, roc_auc: 0.752\n",
      "Days: 50: recall: 0.837, f1: 0.770, precision: 0.713, roc_auc: 0.750\n",
      "Days: 51: recall: 0.838, f1: 0.770, precision: 0.713, roc_auc: 0.750\n",
      "Days: 52: recall: 0.840, f1: 0.770, precision: 0.711, roc_auc: 0.749\n",
      "Days: 53: recall: 0.844, f1: 0.770, precision: 0.707, roc_auc: 0.748\n",
      "Days: 54: recall: 0.849, f1: 0.769, precision: 0.703, roc_auc: 0.746\n",
      "Days: 55: recall: 0.855, f1: 0.769, precision: 0.699, roc_auc: 0.744\n",
      "Days: 56: recall: 0.859, f1: 0.769, precision: 0.696, roc_auc: 0.742\n",
      "Days: 57: recall: 0.863, f1: 0.768, precision: 0.691, roc_auc: 0.739\n",
      "Days: 58: recall: 0.865, f1: 0.768, precision: 0.691, roc_auc: 0.739\n",
      "Days: 59: recall: 0.867, f1: 0.768, precision: 0.689, roc_auc: 0.738\n",
      "Days: 60: recall: 0.872, f1: 0.768, precision: 0.686, roc_auc: 0.736\n",
      "Days: 61: recall: 0.874, f1: 0.767, precision: 0.683, roc_auc: 0.734\n",
      "Days: 62: recall: 0.877, f1: 0.766, precision: 0.681, roc_auc: 0.733\n",
      "Days: 63: recall: 0.879, f1: 0.765, precision: 0.678, roc_auc: 0.730\n",
      "Days: 64: recall: 0.881, f1: 0.764, precision: 0.675, roc_auc: 0.728\n",
      "Days: 65: recall: 0.882, f1: 0.764, precision: 0.674, roc_auc: 0.728\n",
      "Days: 66: recall: 0.883, f1: 0.764, precision: 0.673, roc_auc: 0.727\n",
      "Days: 67: recall: 0.886, f1: 0.763, precision: 0.670, roc_auc: 0.725\n",
      "Days: 68: recall: 0.888, f1: 0.762, precision: 0.668, roc_auc: 0.723\n",
      "Days: 69: recall: 0.891, f1: 0.761, precision: 0.665, roc_auc: 0.721\n",
      "Days: 70: recall: 0.894, f1: 0.760, precision: 0.662, roc_auc: 0.719\n",
      "Days: 71: recall: 0.896, f1: 0.759, precision: 0.659, roc_auc: 0.716\n",
      "Days: 72: recall: 0.897, f1: 0.759, precision: 0.659, roc_auc: 0.716\n",
      "Days: 73: recall: 0.898, f1: 0.759, precision: 0.657, roc_auc: 0.715\n",
      "Days: 74: recall: 0.899, f1: 0.757, precision: 0.654, roc_auc: 0.712\n",
      "Days: 75: recall: 0.901, f1: 0.757, precision: 0.652, roc_auc: 0.710\n",
      "Days: 76: recall: 0.903, f1: 0.756, precision: 0.650, roc_auc: 0.708\n",
      "Days: 77: recall: 0.905, f1: 0.754, precision: 0.647, roc_auc: 0.705\n",
      "Days: 78: recall: 0.907, f1: 0.753, precision: 0.644, roc_auc: 0.703\n",
      "Days: 79: recall: 0.907, f1: 0.753, precision: 0.643, roc_auc: 0.702\n",
      "Days: 80: recall: 0.908, f1: 0.752, precision: 0.642, roc_auc: 0.701\n",
      "Days: 81: recall: 0.910, f1: 0.752, precision: 0.640, roc_auc: 0.699\n",
      "Days: 82: recall: 0.912, f1: 0.751, precision: 0.638, roc_auc: 0.697\n",
      "Days: 83: recall: 0.914, f1: 0.750, precision: 0.636, roc_auc: 0.695\n",
      "Days: 84: recall: 0.916, f1: 0.749, precision: 0.634, roc_auc: 0.693\n",
      "Days: 85: recall: 0.919, f1: 0.748, precision: 0.631, roc_auc: 0.690\n",
      "Days: 86: recall: 0.920, f1: 0.748, precision: 0.630, roc_auc: 0.690\n",
      "Days: 87: recall: 0.921, f1: 0.747, precision: 0.629, roc_auc: 0.689\n",
      "Days: 88: recall: 0.923, f1: 0.746, precision: 0.626, roc_auc: 0.686\n",
      "Days: 89: recall: 0.925, f1: 0.745, precision: 0.624, roc_auc: 0.683\n",
      "Days: 90: recall: 0.927, f1: 0.744, precision: 0.622, roc_auc: 0.681\n"
     ]
    }
   ],
   "source": [
    "rfm_train_data_180_days = pd.read_parquet(\"data/rfm.train.left.180.right.95.parquet.gzip\")\n",
    "balanced_rfm_train_data_180_days = data_balancing_by_col(rfm_train_data_180_days)\n",
    "\n",
    "for day in range(30,91):\n",
    "    print(f\"Days: {day}:\", end=' ')\n",
    "    calc_metrics(calc_predictions_by_days(data_rfm=balanced_rfm_train_data_180_days, days_for_die=day))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если человек 46 дней не совершает покупки, то значит уже не вернется\n",
    "\n",
    "**recall: 0.819** <br/>\n",
    "**f1: 0.771** <br/>\n",
    "**precision: 0.729** <br/>\n",
    "**roc_auc: 0.757** <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff: 29320. Rfm: 347497, Raw: 444541\n"
     ]
    }
   ],
   "source": [
    "from btyd.utils import calculate_alive_path, expected_cumulative_transactions\n",
    "import pickle\n",
    "\n",
    "def calc_alive_path_val(partner):\n",
    "    t = rfm[\"T\"].max()\n",
    "    transactions = data[data[\"partner\"]==partner]\n",
    "    datetime_col = \"rep_date\"\n",
    "    freq=\"D\"\n",
    "\n",
    "    customer_history = transactions[[datetime_col]].copy()\n",
    "    customer_history.index = pd.DatetimeIndex(customer_history[datetime_col])\n",
    "    customer_history[\"transactions\"] = 1\n",
    "    customer_history = customer_history.resample(freq).sum()\n",
    "    path = calculate_alive_path(model, transactions, datetime_col, t, freq)\n",
    "    return path.iloc[-1][0]\n",
    "\n",
    "data = pd.read_parquet(\"data/raw.train.left.180.right.95.parquet.gzip\")\n",
    "rfm = pd.read_parquet(\"data/rfm.train.left.180.right.95.parquet.gzip\")\n",
    "model = pickle.load(open(\"model/beta.geo.model.180.days.part.pkl\", \"rb\"))\n",
    "\n",
    "persons_data = set(data[\"partner\"])\n",
    "persons_rfm = set(rfm.index.to_list())\n",
    "persons_intersected = persons_data.intersection(persons_rfm)\n",
    "persons_intersected_first_100 = list(persons_intersected)[:100]\n",
    "\n",
    "print(f\"Diff: {len(persons_rfm.difference(persons_data))}. Rfm: {len(persons_rfm)}, Raw: {len(persons_data)}\")\n",
    "\n",
    "rfm_100 = rfm.loc[persons_intersected_first_100]\n",
    "rfm_100 = rfm_100.reset_index()\n",
    "rfm_100[\"alive_prob\"] = rfm_100['partner'].apply(lambda p: calc_alive_path_val(partner=p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.163, f1: 0.252, precision: 0.565, roc_auc: 0.331\n",
      "recall: 0.237, f1: 0.333, precision: 0.559, roc_auc: 0.244\n",
      "recall: 0.450, f1: 0.550, precision: 0.706, roc_auc: 0.350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45, 0.549618320610687, 0.7058823529411765, 0.35)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfm_100[PRED] = rfm_100[\"alive_prob\"].apply(lambda x: x >= .5)\n",
    "calc_metrics(rfm_100)\n",
    "\n",
    "rfm_100[PRED] = rfm_100[\"alive_prob\"].apply(lambda x: x >= .3)\n",
    "calc_metrics(rfm_100)\n",
    "\n",
    "rfm_100[PRED] = rfm_100[\"alive_prob\"].apply(lambda x: x >= .1)\n",
    "calc_metrics(rfm_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"model/beta.geo.model.180.days.part.pkl\", \"rb\"))\n",
    "rfm = pd.read_parquet(\"data/rfm.train.left.180.right.95.parquet.gzip\")\n",
    "rfm = rfm.reset_index()\n",
    "\n",
    "rfm[\"pursaches\"] = rfm.apply(lambda row: model.conditional_expected_number_of_purchases_up_to_time(\n",
    "    row.loc[\"partner\"], row.loc[\"frequency\"], row.loc[\"recency\"], row.loc[\"T\"]\n",
    "), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.450, f1: 0.550, precision: 0.706, roc_auc: 0.350\n",
      "recall: 0.450, f1: 0.550, precision: 0.706, roc_auc: 0.350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45, 0.549618320610687, 0.7058823529411765, 0.35)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfm[PRED] = rfm[\"pursaches\"].apply(lambda x: x > 0)\n",
    "calc_metrics(rfm_100)\n",
    "\n",
    "rfm[PRED] = rfm[\"pursaches\"].apply(lambda x: x > 3)\n",
    "calc_metrics(rfm_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Из 347497 строк 21529 - дубликаты\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def rfm_prepare(data_rfm: pd.DataFrame) -> pd.DataFrame:\n",
    "    local_data = data_rfm.copy()\n",
    "    local_data[\"first_buy_last_buy_delta_days\"] = local_data[\"last_buy\"] - local_data[\"first_buy\"] \n",
    "    local_data[\"first_buy_last_buy_delta_days\"] = local_data[\"first_buy_last_buy_delta_days\"].apply(lambda x: x.days)\n",
    "    local_data = data_rfm.drop([\"first_buy\", \"last_buy\"], axis=1)\n",
    "    return local_data\n",
    "\n",
    "rfm = pd.read_parquet(\"data/rfm.train.left.180.right.95.parquet.gzip\")\n",
    "\n",
    "rfm = rfm_prepare(rfm)\n",
    "print(f\"Из {rfm.shape[0]} строк {rfm[rfm.duplicated()].shape[0]} - дубликаты\")\n",
    "rfm = rfm.drop_duplicates()\n",
    "y = rfm[TARGET]\n",
    "X = rfm.drop(TARGET, axis=1)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.894, f1: 0.848, precision: 0.807, roc_auc: 0.716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8937113098162524,\n",
       " 0.8479833672080404,\n",
       " 0.8067071115604988,\n",
       " 0.7157919965680777)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfm_test = pd.read_parquet(\"data/rfm.test.left.180.right.95.parquet.gzip\")\n",
    "\n",
    "rfm_test = rfm_prepare(rfm_test)\n",
    "predictions = model.predict(rfm_test.drop(TARGET, axis=1))\n",
    "rfm_test[PRED] = predictions\n",
    "\n",
    "calc_metrics(rfm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 0.894, f1: 0.759, precision: 0.659, roc_auc: 0.716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8939522456423088, 0.7588472477229362, 0.6592181391712275, 0.715912464481106)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfm_test = pd.read_parquet(\"data/rfm.test.left.180.right.95.parquet.gzip\")\n",
    "\n",
    "rfm_test = data_balancing_by_col(rfm_prepare(rfm_test))\n",
    "predictions = model.predict(rfm_test.drop(TARGET, axis=1))\n",
    "rfm_test[PRED] = predictions\n",
    "\n",
    "calc_metrics(rfm_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
